{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5tpTcbh9Y0S"
   },
   "source": [
    "# Homework: word-level embedding and query search engine\n",
    "\n",
    "Broadly speaking, your task will be to:\n",
    "- Train a fasttext model on War and Peace by Leo Tolstoy\n",
    "- Finetune (continue training) obtained embeddings using quora questions in order to account for new tokens and ever-changing semantic meaning\n",
    "- Embed existing quora questions and store them for later use\n",
    "- Implement a way to search the closest match among quora questions for an unseen text\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9wbkl1_K2pX"
   },
   "source": [
    "# Training initial embeddings (4 points)\n",
    "\n",
    "Your first task will be to train embeddings on War and Peace by Leo Tolstoy. using a word2vec extention called fasttext.\n",
    "\n",
    "There are certain differences with a vanilla word2vec, which you can read about [here](https://github.com/facebookresearch/fastText?tab=readme-ov-file), for example, but for all intents and purposes it is easier to treat fasttext as a word2vec on steroids.\n",
    "\n",
    "The most important difference, arguably, is that the model vocabulary is constructed not only using word-level tokens, but also their n-grams e.g. for a word \"peace\" the resulting tokens using 2-grams would look like (peace, pe, ea, ac, ce).\n",
    "\n",
    "This allows the model not only to handle out-of-vocabulary words by combining the corresponding subword tokens, but also to lift the need for word normalization (stemming, lemmatization) in some cases, preserving semantics in morphology-rich languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /Users/hq-vmvvwffx99/Library/Caches/pypoetry/virtualenvs/6-nlp-58CPPIPe-py3.12/lib/python3.12/site-packages (1.3.8)\n",
      "Requirement already satisfied: faiss-cpu in /Users/hq-vmvvwffx99/Library/Caches/pypoetry/virtualenvs/6-nlp-58CPPIPe-py3.12/lib/python3.12/site-packages (1.9.0.post1)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/hq-vmvvwffx99/Library/Caches/pypoetry/virtualenvs/6-nlp-58CPPIPe-py3.12/lib/python3.12/site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in /Users/hq-vmvvwffx99/Library/Caches/pypoetry/virtualenvs/6-nlp-58CPPIPe-py3.12/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, tqdm, regex, numpy, joblib, click, smart-open, scipy, nltk, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed click-8.1.7 gensim-4.3.3 joblib-1.4.2 nltk-3.9.1 numpy-1.26.4 regex-2024.11.6 scipy-1.13.1 smart-open-7.0.5 tqdm-4.67.1 wrapt-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode faiss-cpu gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: /Users/hq-vmvvwffx99/Library/Caches/pypoetry/virtualenvs/6-nlp-58CPPIPe-py3.12/lib/python3.12/site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aQvA2lKuIH0I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/hq-\n",
      "[nltk_data]     vmvvwffx99/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Union, List, Tuple, Callable\n",
    "\n",
    "import nltk\n",
    "import faiss\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "# Note that in order to do the computations trully determenistic\n",
    "# it is required to run the code using a sigle thread\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj_4h-PRfY73"
   },
   "source": [
    "Load war and peace text file from Gutenberg project repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeZn-hIDKGsn",
    "outputId": "43189430-ca5c-4cbc-9242-cb09cba309e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-01 19:03:13--  https://www.gutenberg.org/files/2600/2600-0.txt\n",
      "Распознаётся www.gutenberg.org (www.gutenberg.org)… 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Подключение к www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 3359405 (3,2M) [text/plain]\n",
      "Сохранение в: «war_and_peace_raw.txt»\n",
      "\n",
      "war_and_peace_raw.t 100%[===================>]   3,20M  2,11MB/s    за 1,5s    \n",
      "\n",
      "2024-12-01 19:03:15 (2,11 MB/s) - «war_and_peace_raw.txt» сохранён [3359405/3359405]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/2600/2600-0.txt\" -O war_and_peace_raw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3XzDrMW_K_al"
   },
   "outputs": [],
   "source": [
    "wap_raw_file_path = 'war_and_peace_raw.txt'\n",
    "wap_cleaned_file_path = 'war_and_peace_cleaned.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oudD5cgf4HW"
   },
   "source": [
    "Note that the text contains information redundant for the task, so we need to preprocess the file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiOOgPlrf8d5",
    "outputId": "840fe778-87c4-4965-9a6d-9ae1437a72fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of War and Peace, by Leo Tolstoy\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head war_and_peace_raw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mAgEH8hTLMGZ"
   },
   "outputs": [],
   "source": [
    "# Load the text from the file\n",
    "with open(wap_raw_file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Remove the header and footer\n",
    "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK WAR AND PEACE ***\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK WAR AND PEACE ***\"\n",
    "text = text.split(start_marker, 1)[-1]  # Remove everything before the start marker\n",
    "text = text.split(end_marker, 1)[0]     # Remove everything after the end marker\n",
    "\n",
    "# Split the text into lines and filter out unwanted ones\n",
    "lines = text.split(\"\\n\")\n",
    "filtered_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    # Remove lines that are empty or contain chapter headers or non-content information\n",
    "    if not line or line.startswith(\"CHAPTER\") or line.isupper():\n",
    "        continue\n",
    "    filtered_lines.append(line)\n",
    "\n",
    "# Join the lines back into a single string\n",
    "cleaned_text = \"\\n\".join(filtered_lines[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dva_nQlg3tq"
   },
   "source": [
    "The text contains some non-ascii characters for historical reasons. One way to deal with it is to cast them to the closest ascii equivalent.\n",
    "\n",
    "Note that it is not always required and in some cases may affect the model performance in a bad way.\n",
    "\n",
    "*In short, if you feel that these characters are needed, leave them be.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xgYaB7n-Ljmv"
   },
   "outputs": [],
   "source": [
    "# Normalize to ASCII\n",
    "cleaned_text_ascii = unidecode(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TJGtNtghwki"
   },
   "source": [
    "Another important thing is to know your data and how the model trains its weights.\n",
    "\n",
    "In our case, word2vec (fasttext) processes the text line by line, traversing the line with a defined window to represent the context. However, the Gutenberg War and Peace text is split into lines in a somewhat arbitrary way, so that the words on a particular line may not necesserily belong to the same sentence and thus the context window will erroneously treat them as contextually-dependent.\n",
    "\n",
    "One way to fix it is to tokenize the text into sentences first. Luckily for us, the whole War and Peace text fits into the RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6joylcPhuxS",
    "outputId": "754e9154-c52f-471b-fce2-ff957e8c2a7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Well, Prince, so Genoa and Lucca are now just family estates of the\\nBuonapartes.',\n",
       " \"But I warn you, if you don't tell me that this means war,\\nif you still try to defend the infamies and horrors perpetrated by that\\nAntichrist--I really believe he is Antichrist--I will have nothing\\nmore to do with you and you are no longer my friend, no longer my\\n'faithful slave,' as you call yourself!\",\n",
       " 'But how do you do?',\n",
       " 'I see I\\nhave frightened you--sit down and tell me all the news.\"',\n",
       " 'It was in July, 1805, and the speaker was the well-known Anna Pavlovna\\nScherer, maid of honor and favorite of the Empress Marya Fedorovna.',\n",
       " 'With these words she greeted Prince Vasili Kuragin, a man of high\\nrank and importance, who was the first to arrive at her reception.',\n",
       " 'Anna\\nPavlovna had had a cough for some days.',\n",
       " 'She was, as she said, suffering\\nfrom la grippe; grippe being then a new word in St. Petersburg, used\\nonly by the elite.',\n",
       " 'All her invitations without exception, written in French, and delivered\\nby a scarlet-liveried footman that morning, ran as follows:\\n\"If you have nothing better to do, Count (or Prince), and if the\\nprospect of spending an evening with a poor invalid is not too terrible,\\nI shall be very charmed to see you tonight between 7 and 10--Annette\\nScherer.\"',\n",
       " '\"Heavens!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into sentences\n",
    "sentences = nltk.sent_tokenize(cleaned_text_ascii)\n",
    "\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvi9Je9ukb5k"
   },
   "source": [
    "As you can see, there are many unwanted characters in each sentence: arbitrary newlines (\\n), double hyphens (--) etc.\n",
    "\n",
    "For the task at hand (learning fasttext embeddings), we might want to preprocess the text i.e. get rid of punctuation and redundant symbols, lowercase each word. We could also apply stemming or lemmatization, but fasttext gives us the freedom not to do so, preserving some morphology-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SAS8gix8MmJx"
   },
   "outputs": [],
   "source": [
    "def clean_wap_sentence(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a sentence by performing the following operations:\n",
    "    1. Replaces newline characters (`\\n`) with a space.\n",
    "    2. Replaces double hyphens (`--`) with a space.\n",
    "    3. Removes all punctuation except for apostrophes.\n",
    "    4. Converts the sentence to lowercase.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned sentence.\n",
    "    \"\"\"\n",
    "    sentence = str.lower((sentence.replace(\"\\n\", \" \").replace(\"--\", \" \").strip()))\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]', ' ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xFtKcY1FLltn"
   },
   "outputs": [],
   "source": [
    "# Preprocess each sentence\n",
    "processed_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    if sentence:\n",
    "        processed_sentences.append(clean_wap_sentence(sentence).strip().replace(\"  \", \" \"))\n",
    "\n",
    "# Join processed sentences with newlines to match expected gensim format\n",
    "final_text = \"\\n\".join(processed_sentences)\n",
    "\n",
    "with open(wap_cleaned_file_path, 'w') as outfile:\n",
    "    outfile.write(final_text)\n",
    "\n",
    "# Keep sentence count for gensim model parameter setting\n",
    "wap_sentences_count = len(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4OMLve2Nu29",
    "outputId": "fbe76b06-cb07-4a19-fc23-3382cf888867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well prince so genoa and lucca are now just family estates of the buonapartes\n",
      "but i warn you if you don t tell me that this means war if you still try to defend the infamies and horrors perpetrated by that antichrist i really believe he is antichrist i will have nothing more to do with you and you are no longer my friend no longer my faithful slave  as you call yourself\n",
      "but how do you do\n",
      "i see i have frightened you sit down and tell me all the news\n"
     ]
    }
   ],
   "source": [
    "# Take a look at what we obtained so far\n",
    "!sed -n 1,4p war_and_peace_cleaned.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uH0e8AMmxTM"
   },
   "source": [
    "Now that we have the preprocessed data, let's train our model at last!\n",
    "\n",
    "For this task we will use [Gensim's implementation](https://radimrehurek.com/gensim/models/fasttext.html) of Fasttext. It has a little overhead as compared to the [vanilla implementation](https://fasttext.cc/), but a really nice API which also supports finetuning existing models.\n",
    "\n",
    "\n",
    "**Please read carefully these notes in order to pass the tests:**\n",
    "\n",
    "0. Set the learning rate to 3e-2. Gensim's fasttextwill linearly decrease this value as epochs go on.\n",
    "1. You are free to experiment with the vector size,\n",
    "but know that it will drastically affect the performance.\n",
    "Values below 64 may result in embeddings to struggle to correctly represent the words.\n",
    "Values above 300 will slow you down and will likely not offer any compensation for that.\n",
    "We recommend a value of 100 to safely pass the test as a happy mean\n",
    "2. Be sure to use skip-gram mode. It usually gives a better semantic representation of the learned vectors at the expense of a relatively slow training.\n",
    "3. 5 epochs is likely enough for the amount of data you have, but feel free to experiment.\n",
    "4. Remember to set the seed, even though it will likely not make you model fully determenistic if you use more than one thread.\n",
    "5. Window size is an important parameter. The wider the window, the broader sentence-level context a given embedding captures at the expense of own semantic uniqueness. Feel free to experiment. The window size of 5 will likely be a good starting point to safely pass the assert wall.\n",
    "\n",
    "Note that we do not use any train\\test split. You, the human, is the ultimate referee for this task. If you feel that that the embeddings make sense (and they pass the test), then so be it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3e-2\n",
    "epoch_cnt = 5\n",
    "vector_size = 256\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9K63bW7jVlJS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6551"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use PathLineSentence to stream data from the file\n",
    "wap_sentences = PathLineSentences(wap_cleaned_file_path)\n",
    "\n",
    "# Train the FastText model using Gensim\n",
    "model = FastText(\n",
    "    sentences=wap_sentences,\n",
    "    vector_size=vector_size,\n",
    "    alpha=alpha,\n",
    "    window=window,\n",
    "    sg=1,  # Skip-gram mode\n",
    "    epochs=epoch_cnt,\n",
    "    min_count=5,\n",
    "    seed=42\n",
    ")\n",
    "model_vocabulary = set(model.wv.key_to_index.keys())\n",
    "len(model_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"wap_fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff7lhpvi82XM"
   },
   "source": [
    "You can play around with the model you trained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8WtJNHJ37je",
    "outputId": "936e0ff3-81ad-4447-85b8-dc952b3010f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('disgrace', 0.8543407917022705)\n",
      "('religion', 0.8525285720825195)\n",
      "('oblige', 0.842536211013794)\n",
      "('perform', 0.839191198348999)\n",
      "('failure', 0.8386883735656738)\n",
      "('refusal', 0.8376811146736145)\n",
      "('fulfillment', 0.835666835308075)\n",
      "('insufficient', 0.8335513472557068)\n",
      "('secrecy', 0.8331008553504944)\n",
      "('sufficient', 0.8298017382621765)\n"
     ]
    }
   ],
   "source": [
    "print(*model.wv.most_similar('peace', topn=10), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x-XkVDyqwpg"
   },
   "source": [
    "Fasttext allows us to reconstruct out-of-vocabulary words using word n-grams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkuCReZxq425",
    "outputId": "e95d1f46-bd95-415c-f592-8216fab137a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'computation' in model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESLUQDBMrRwr"
   },
   "source": [
    "...even though they may be too far from the learned context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBwOjSQZrA5k",
    "outputId": "9b6655ce-750a-4b5a-b3c3-e9a9b5a13062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deputation', 0.9751371145248413)\n",
      "('combination', 0.9700029492378235)\n",
      "('subordination', 0.9677756428718567)\n",
      "('consultation', 0.9677189588546753)\n",
      "('sensation', 0.962062656879425)\n",
      "('participation', 0.9608920812606812)\n",
      "('determination', 0.9594915509223938)\n",
      "('gravitation', 0.9582380652427673)\n",
      "('confirmation', 0.9566984176635742)\n",
      "('temptation', 0.9562721252441406)\n"
     ]
    }
   ],
   "source": [
    "print(*model.wv.most_similar('computation', topn=10), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wB9bwgA53sFJ",
    "outputId": "0f045292-9ece-4cbd-bc32-db070ff98d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "# Degugging area\n",
    "model_vocabulary = set(model.wv.key_to_index.keys())\n",
    "most_similar_to_peace = list(zip(*model.wv.most_similar('peace', topn=50)))[0]\n",
    "\n",
    "assert model.vector_size >= 64 and model.vector_size <= 300, 'Please check your embedding size.'\n",
    "assert model.sg == 1, 'Please use skip-gram method for consistency. Also, despite being faster, CBOW usually generates inferior word embeddings for rare words.'\n",
    "assert model.alpha == 3e-2, 'It is expected that you overwrite the default alpha for this task.'\n",
    "assert len(model_vocabulary) > 5500 and len(model_vocabulary) < 7500, 'There is something wrong with your tokenization. Check the pipeline and use a default *ngram=1*'\n",
    "assert 'religion' in most_similar_to_peace, 'Embeddings look odd. Make sure to follow instructions!'\n",
    "\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB4k_ER8XYtJ"
   },
   "source": [
    "# Finetune using quora dataset (3 points)\n",
    "\n",
    "Now that we have embeddings representing the semantic space as was perceived by !Leo Tolstoy, let's move on to more up-to-date representations.\n",
    "\n",
    "One problem is that our model's vocabulary lacks knowledge about modern day words. Even though we can reconstruct said words using fasttext n-grams, they lack contextual meaning.\n",
    "Another problem is that the context in which certain words are used could have changed throughout the years.\n",
    "\n",
    "Hence, our task would be not to overwrite the learned vectors, but to enrich them with some fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hs7FKYp6Hl4b",
    "outputId": "2ce1348a-50d9-4843-f7ab-4e8c675517f4"
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "#!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora_raw.txt\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBNa4AViHuxO",
    "outputId": "f4972313-c382-4bc8-a230-d5f02a93f4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I get back with my ex even though she is pregnant with another guy's baby?\n",
      "What are some ways to overcome a fast food addiction?\n",
      "Who were the great Chinese soldiers and leaders who fought in WW2?\n",
      "What are ZIP codes in the Bay Area?\n",
      "Why was George RR Martin critical of JK Rowling after losing the Hugo award?\n"
     ]
    }
   ],
   "source": [
    "# Glimpse at the quora dataset\n",
    "!sed -n 1,5p quora_raw.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epNDC2bgsCqR"
   },
   "source": [
    "As before, we need to standardize the data for training.\n",
    "\n",
    "Luckily for us, each line already consists of a complete sentence (even better, in ASCII!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhGLucM6Hzp_",
    "outputId": "f58f5e24-5971-49f6-d60d-00187ff1e7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines processed: 537272\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess a single line\n",
    "def preprocess_line(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a single line of text by:\n",
    "    1. Removing all punctuation except apostrophes.\n",
    "    2. Converting the text to lowercase.\n",
    "\n",
    "    Args:\n",
    "        line (str): The input line of text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed line of text.\n",
    "    \"\"\"\n",
    "    line = str.lower((line.replace(\"\\n\", \" \").replace(\"--\", \" \").strip()))\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]', ' ', line)\n",
    "\n",
    "\n",
    "# Preprocess the text file line by line\n",
    "def preprocess_file(input_file_path: str, output_file_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Reads a text file line by line, preprocesses each line,\n",
    "    and writes the preprocessed lines to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): Path to the input text file.\n",
    "        output_file_path (str): Path to the output text file where preprocessed lines will be saved.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of lines processed in the input file.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    final_text = []\n",
    "    for line in text.split('\\n'):\n",
    "        final_text.append(preprocess_line(line))\n",
    "\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(final_text))\n",
    "\n",
    "    return len(final_text)\n",
    "\n",
    "\n",
    "# Paths to the input and output files\n",
    "quora_raw_file_path = 'quora_raw.txt'\n",
    "quora_processed_file_path = 'quora_processed.txt'\n",
    "\n",
    "# Preprocess the file and count the number of lines processed\n",
    "quora_sentences_count = preprocess_file(quora_raw_file_path, quora_processed_file_path)\n",
    "\n",
    "print(f'Lines processed: {quora_sentences_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDq2kwWvx6V1"
   },
   "source": [
    "The catch here is that we continue trainining the model using a larger dataset with a pretty different thematics, so in order not to completely overwrite existing War and Peace \"vibe\" it may be a good idea to proceed at where we left off previously (learning rate-wise) and/or not to be too crazy about the number of epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wap_sentences = PathLineSentences(wap_cleaned_file_path)\n",
    "quora_sentences = PathLineSentences(quora_processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "s848tJDOWw6m"
   },
   "outputs": [],
   "source": [
    "# As before, use gensim's tools to iterate over the lines\n",
    "quora_sentences = PathLineSentences(quora_processed_file_path)\n",
    "\n",
    "# Load the model in order to finetune it\n",
    "finetuned_model = FastText.load(\"wap_fasttext_model.bin\")\n",
    "finetuned_model.alpha=model.min_alpha\n",
    "finetuned_model.vector_size=vector_size\n",
    "\n",
    "# Update existing vocabulary with the new sentences\n",
    "finetuned_model.build_vocab(corpus_iterable=quora_sentences, update=True)\n",
    "\n",
    "# Continue training the model on the new data\n",
    "# Remember to correctly pass the parameters\n",
    "finetuned_model.train(\n",
    "    corpus_iterable=quora_sentences,\n",
    "    total_examples=finetuned_model.corpus_count,\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "finetuned_model.save(\"wap_quora_fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2fb7euXJe-u",
    "outputId": "1c3acb73-6b1a-471d-d423-bd4e0317a999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new words added: 22937\n",
      "New words: ['googled', 'planet', 'declaration', 'suisse', 'barrister', 'journal', 'mixture', 'cereals', 'innocence', 'retweet']\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_vocabulary = set(finetuned_model.wv.key_to_index.keys())\n",
    "new_words = finetuned_model_vocabulary - model_vocabulary\n",
    "\n",
    "print(\"Number of new words added:\", len(new_words))\n",
    "print(\"New words:\", list(new_words)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Lryyye8IG2X6"
   },
   "outputs": [],
   "source": [
    "def print_comparison(\n",
    "    model1: Union[FastText, None],\n",
    "    model2: Union[FastText, None],\n",
    "    word: str = 'peace',\n",
    "    top_n: int = 10\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A utility function that compares the top-N most similar words for a given query word across two FastText models.\n",
    "    Prints the comparison in a tabular format to visually assess changes after finetuning.\n",
    "\n",
    "    Args:\n",
    "        model1 (FastText | None): The first FastText model (original or pre-finetuning).\n",
    "        model2 (FastText | None): The second FastText model (post-finetuning).\n",
    "        word (str, optional): The word for which similar words are retrieved.\n",
    "        top_n (int, optional): The number of top similar words to compare.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints results directly to the console.\n",
    "    \"\"\"\n",
    "    q0 = model1.wv.most_similar(word, topn=top_n)\n",
    "    q1 = model2.wv.most_similar(word, topn=top_n)\n",
    "\n",
    "    # Print the header\n",
    "    print(f'Query: {word}\\n')\n",
    "    print(f\"{'pos':<5} {'model_1':<15} {'score_1':<10} {'model_2':<15} {'score_2':<10}\")\n",
    "    print(\"-\" * (5 + 15 + 10 + 15 + 10))\n",
    "\n",
    "    # Print each entry up to top_n\n",
    "    for pos, (word0, score0), (word1, score1) in zip(range(top_n), q0, q1):\n",
    "        # Format the scores to 4 decimal places\n",
    "        print(f\"{pos:<5} {word0:<15} {score0:.4f}     {word1:<15} {score1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EpYIYHqIY0C",
    "outputId": "32b0e4c0-df85-4c15-e8e3-1caace5fdd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: war\n",
      "\n",
      "pos   model_1         score_1    model_2         score_2   \n",
      "-------------------------------------------------------\n",
      "0     victory         0.8089     warg            0.9418\n",
      "1     campaign        0.8075     codevita        0.9202\n",
      "2     vital           0.8066     ministry        0.9198\n",
      "3     discussion      0.8050     revolution      0.9184\n",
      "4     commit          0.8031     abolition       0.9157\n",
      "5     entertainment   0.8021     evolution       0.9116\n",
      "6     revolution      0.7996     communication   0.9113\n",
      "7     description     0.7981     declaration     0.9112\n",
      "8     conclusion      0.7971     codec           0.9111\n",
      "9     discuss         0.7971     dilution        0.9111\n"
     ]
    }
   ],
   "source": [
    "# Call the function to display results\n",
    "print_comparison(model, finetuned_model, word='war', top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClPM4Nx-FENm",
    "outputId": "b8c67085-fefe-4cf8-fc35-02c63e9ebf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gratz!\n"
     ]
    }
   ],
   "source": [
    "# The usual assert wall for debugging and testing\n",
    "assert finetuned_model.alpha <= model.min_alpha, 'setting a learning rate too high will result in model forgetting previous information, especially when finetuning on a larger dataset!'\n",
    "assert len(new_words) > 20_000 and len(new_words) < 30_000, 'There is likely something wrong with your preprocessing pipeline. There must be more new words after finetuning!'\n",
    "assert 'religion' in list(zip(*finetuned_model.wv.most_similar('peace', topn=50)))[0], 'Embeddings changed dramatically! Consider setting lower start_apha and fewer epochs.'\n",
    "assert 'evolution' in list(zip(*finetuned_model.wv.most_similar('war', topn=50)))[0], 'Embeddings didn\\'t change as was expected! Consider setting higher start_apha and more epochs, check *total_examples* param.'\n",
    "\n",
    "print('Gratz!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNNdvFcQNRIm"
   },
   "source": [
    "In practice, you may want to experiment with different fine-tuning strategies e.g. varying the alpha parameter, training for more epochs, or even trying to \"freeze\" some embeddings while updating only certain parts of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMY77szU6G6J"
   },
   "source": [
    "# Implement a way to search similar quora questions given a new query (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu8PJZxk6OtW"
   },
   "source": [
    "## Straightforward numpy array-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmBNaucPb43q"
   },
   "source": [
    "Next, if we are to do a semantic search engine, it is required to be able to match a new sentence to a single vector. The easiest way to do so is to average individual word-level embeddings.\n",
    "\n",
    "Alternatively, you may try out pooling (min, max, mean, softmax) strategies and [TF-IDF](https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/) . TF-IDF approach requires to precalculate IDF scores across existing corpus and multiply by a TF for a given word in a given sentence.\n",
    "\n",
    "Also, it is expected that you will use cosine similarity as a vector distance metric later, so it might be a good idea to normalize all the vectors before putting them into a database since it will effectively turn a cosine similarity calculation into taking a dot product, which will speed the things up:\n",
    "$$\n",
    "cos(\\phi) = \\frac{<u, v>}{|u||v|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object simple_tokenize at 0x16965e0c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Love peace\"\n",
    "processed_text = preprocess_line(question)\n",
    "tokenized_text = tokenize(processed_text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "kohkQMO7T2O6"
   },
   "outputs": [],
   "source": [
    "def normalize_vector(embedding: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes a given vector to have unit length.\n",
    "\n",
    "    Args:\n",
    "        embedding (np.ndarray): A NumPy array representing the vector to normalize.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A normalized vector with unit length.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    if norm == 0:\n",
    "        return embedding\n",
    "    return embedding / norm\n",
    "\n",
    "\n",
    "\n",
    "def get_text_embedding(text: str, model: FastText) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the embedding for a given text using a pre-trained FastText model.\n",
    "    The embedding is the mean of word embeddings for words in the text, normalized to unit length.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to embed.\n",
    "        model (FastText): The FastText model to use for generating embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized embedding for the input text.\n",
    "                    If no valid words are found in the text, returns a zero vector.\n",
    "    \"\"\"\n",
    "    processed_text = preprocess_line(text)\n",
    "    tokenized_text = tokenize(processed_text)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "    \n",
    "    if not embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    return normalize_vector(np.mean(embeddings, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxgO1OC6g76i",
    "outputId": "7118cf82-8a3f-413a-9f72-04b5c72374c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "# Debugging and testing area\n",
    "question = \"Love peace\"\n",
    "text_embedding = get_text_embedding(question, finetuned_model)\n",
    "dummy_word_embedding = (finetuned_model.wv['love'] + finetuned_model.wv['peace']) / 2\n",
    "\n",
    "assert text_embedding.shape[0] == finetuned_model.wv.vector_size, \"Check axis along which values are aggregated\"\n",
    "assert all(normalize_vector(dummy_word_embedding) == text_embedding), \"You need to return a normalized mean embedding for now, nothing fancy!\"\n",
    "\n",
    "print('Good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feR2nwsPx_Hj"
   },
   "source": [
    "Finally, let's implement a similar sentence search.\n",
    "\n",
    "Your task will be to match each quora question to an embedding and store these vectors in a way easy enough to conduct a search operation.\n",
    "\n",
    "In order this to look like a search engine, new user input should also be converted to an embedding in order to perform, well, a search in our existing embedding storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzBoJNuFzjuA"
   },
   "source": [
    "First, let's do it in a straightforward way: create a numpy array where existing quora questions embeddings will be stored.\n",
    "\n",
    "Be sure to apply the same preprocessing we used during the model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zOg0BKJOSL1m"
   },
   "outputs": [],
   "source": [
    "def create_embeddings_storage(quora_file: str, model: FastText) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a NumPy array of normalized embeddings for all questions in a dataset.\n",
    "\n",
    "    Each line in the input file represents a question, which is preprocessed using\n",
    "    'preprocess_line' function and embedded using 'get_text_embedding' function.\n",
    "\n",
    "    Args:\n",
    "        quora_file (str): Path to the file containing the preprocessed questions, one per line.\n",
    "        model (FastText): The FastText model used to generate embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array where each row corresponds to the normalized embedding\n",
    "                    of a question in the dataset. Shape: (num_questions, embedding_dim).\n",
    "    \"\"\"\n",
    "    embeddings_list: List[np.ndarray] = []\n",
    "\n",
    "    with open(quora_file, 'r', encoding='utf-8') as file:\n",
    "      for line in file:\n",
    "        embeddings_list.append(get_text_embedding(line, model))\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array\n",
    "    return np.array(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "pNR8KYpJI4V5"
   },
   "outputs": [],
   "source": [
    "# Create a storage array for normalized embeddings\n",
    "embeddings_storage_np = create_embeddings_storage(\"quora_processed.txt\", finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJlU0KJqWgr1",
    "outputId": "1e3e09c9-9098-45e9-bf16-f38869ab708d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537272, 256)==(537272, 256)\n",
      "Looking good so far!\n"
     ]
    }
   ],
   "source": [
    "print(f\"{embeddings_storage_np.shape}=={(quora_sentences_count, finetuned_model.vector_size)}\")\n",
    "with open(quora_processed_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_embedding = get_text_embedding(line, finetuned_model)\n",
    "        break\n",
    "\n",
    "assert embeddings_storage_np.shape == (quora_sentences_count, finetuned_model.vector_size), \"The vector storage must be of size (num_quora_sentences, embedding_dim)\"\n",
    "assert float(round(embeddings_storage_np[0].mean(),7)) == round(float(test_embedding.mean()), 7), 'Embedding of the first quora question does not correspond to the first enty in the storage!'\n",
    "print('Looking good so far!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "7aXP6AQUS2Gg"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_closest_match_np(query: str, model, embeddings_storage: np.ndarray, k: int = 1) -> Tuple[List[int], List[float]]:\n",
    "    \"\"\"\n",
    "    Finds the closest match(es) to a new question in the database using cosine similarity.\n",
    "\n",
    "    This function preprocesses the input question, calculates its embedding, and then computes\n",
    "    cosine similarities between the new question's embedding and a pre-existing database of embeddings.\n",
    "    It returns the indices of the top-k most similar questions along with their similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The input question to be compared.\n",
    "        model: The trained FastText model used to generate word embeddings.\n",
    "        embeddings_storage (np.ndarray): A numpy array containing the precomputed\n",
    "        and normalized embeddings of all questions in the database.\n",
    "        k (int, optional): The number of closest matches to return. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[float]]:\n",
    "            - List[int]: A list of indices of the top-k most similar questions in the database.\n",
    "            - List[float]: A list of similarity scores corresponding to these top-k matches.\n",
    "    \"\"\"\n",
    "    # Preprocess the query and embed the question\n",
    "    prepro_q = preprocess_line(query)\n",
    "\n",
    "    embedding_q = get_text_embedding(prepro_q, model)\n",
    "    embedding_q = embedding_q / np.linalg.norm(embedding_q)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    # Dot product if the vectors are normalized embeddings\n",
    "    # You might want to look up np.dot\n",
    "    similarity = np.dot(embeddings_storage, embedding_q)\n",
    "\n",
    "    # Get the indices of the top-k most similar questions\n",
    "    # You might want to look up np.argsort\n",
    "    top_k_indices = np.argsort(similarity)[-k:][::-1]\n",
    "    top_k_similarities = similarity[top_k_indices].tolist()\n",
    "\n",
    "    return top_k_indices, top_k_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv_9fI1KaKH_"
   },
   "source": [
    "For now, we will store all the quora questions in RAM using a python list.\n",
    "\n",
    "In practice, you would consider a separate database with a retrieval by index done in constant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBgHNHsEZpQW",
    "outputId": "ede5add8-e385-4762-f197-f207edc6a937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions processed: 537272\n"
     ]
    }
   ],
   "source": [
    "# Store original questions for visual testing\n",
    "quora_questions_list = []\n",
    "with open(quora_raw_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        quora_questions_list.append(line.strip())\n",
    "\n",
    "print(f\"Total questions processed: {len(quora_questions_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zjL1Jv_LdmfU"
   },
   "outputs": [],
   "source": [
    "def fetch_and_display_closest_match(query_function: Callable[..., Tuple[List[int], List[float]]], **kwargs) -> Tuple[List[int], List[float]]:\n",
    "    \"\"\"\n",
    "    A utility function to fetch and display the closest matching questions\n",
    "    from the Quora dataset for a given query.\n",
    "\n",
    "    This function takes a query, uses the provided query function to retrieve\n",
    "    the top-k most similar questions, and prints them along with their similarity scores.\n",
    "\n",
    "    Args:\n",
    "        query_function (Callable): A function that takes query-related parameters and returns\n",
    "                                    the indices of the top-k most similar questions and their similarity scores.\n",
    "                                    It should return a tuple (List[int], List[float]).\n",
    "        **kwargs: Additional arguments to be passed to the query_function, including the query text.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[float]]: A tuple containing:\n",
    "            - A list of indices corresponding to the top-k most similar questions in the dataset.\n",
    "            - A list of similarity scores for each of these questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the top-k indices and their similarity scores from the query function\n",
    "    top_k_indices, top_k_similarities = query_function(**kwargs)\n",
    "\n",
    "    # Fetch the actual questions based on the retrieved indices\n",
    "    top_k_questions = [quora_questions_list[i] for i in top_k_indices]\n",
    "\n",
    "    # Print the query and the top-k results with their similarity scores\n",
    "    print(f\"Query: {kwargs['query']}\")\n",
    "    print(\"\\nTop Matches:\")\n",
    "    for i, (question, similarity) in enumerate(zip(top_k_questions, top_k_similarities), 1):\n",
    "        print(f\"{i}. {question} (Similarity: {similarity:.4f})\")\n",
    "\n",
    "    return top_k_indices, top_k_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmSVUzWzam6O",
    "outputId": "fedac0e1-16b5-44db-95d0-813bcc1c6b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I find inner peace?\n",
      "\n",
      "Top Matches:\n",
      "1. How can I find passion? (Similarity: 0.9946)\n",
      "2. How can I create inner peace? (Similarity: 0.9940)\n",
      "3. How can I find a career mentor? (Similarity: 0.9936)\n",
      "4. How can I find inspiration? (Similarity: 0.9933)\n",
      "5. How can I find a successful mentor? (Similarity: 0.9933)\n",
      "6. How can I find a startup mentor? (Similarity: 0.9933)\n",
      "7. How can I find a programmer mentor? (Similarity: 0.9930)\n",
      "8. How can I find a job? (Similarity: 0.9918)\n",
      "9. How can I find a trustworthy distributor？? (Similarity: 0.9914)\n",
      "10. How can I find a JavaScript programmer mentor? (Similarity: 0.9913)\n"
     ]
    }
   ],
   "source": [
    "# testing area\n",
    "new_question = \"How can I find inner peace?\"\n",
    "\n",
    "top_k_indices, top_k_similarities = fetch_and_display_closest_match(query_function=find_closest_match_np,\n",
    "                                                                    query=new_question, model=finetuned_model,\n",
    "                                                                    embeddings_storage=embeddings_storage_np,\n",
    "                                                                    k=10)\n",
    "\n",
    "assert 446084 in top_k_indices, 'Your embeddings look odd. = ('"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjtzTuA4gnXN"
   },
   "source": [
    "Please, feel free to play with your new search engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTP7pViAgxBk",
    "outputId": "43679e64-2a65-447b-f7d2-c6c32501aa93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the future of artificial intelligence?\n",
      "\n",
      "Top Matches:\n",
      "1. What is the future of artificial intelligence? (Similarity: 1.0000)\n",
      "2. What is the scope of Artificial Intelligence? (Similarity: 0.9976)\n",
      "3. What is the future of internet piracy? (Similarity: 0.9969)\n",
      "4. What is the future of Pharmaceutical industry? (Similarity: 0.9968)\n",
      "5. What is the future of Social Media? (Similarity: 0.9967)\n",
      "6. What is the future of Chinese economy? (Similarity: 0.9966)\n",
      "7. What is the future of virtual reality? (Similarity: 0.9965)\n",
      "8. What is the relative density of oil? (Similarity: 0.9964)\n",
      "9. What is the future of Behavioural Economics? (Similarity: 0.9964)\n",
      "10. What is the future of electrical grid? (Similarity: 0.9963)\n"
     ]
    }
   ],
   "source": [
    "new_question = \"What is the future of artificial intelligence?\"\n",
    "\n",
    "_, _ = fetch_and_display_closest_match(query_function=find_closest_match_np,\n",
    "                                       query=new_question, model=finetuned_model,\n",
    "                                       embeddings_storage=embeddings_storage_np,\n",
    "                                       k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72lRXN_Hcd7W",
    "outputId": "56f13280-5572-4198-8b3d-2336d085b287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 6.7784 seconds.\n",
      "Average time: 0.0678 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(iterations):\n",
    "    _, _ = find_closest_match_np('hello there', finetuned_model, embeddings_storage_np, k=10)\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "average_time = time_elapsed / iterations\n",
    "\n",
    "print(f'Time elapsed: {time_elapsed:.4f} seconds.')\n",
    "print(f'Average time: {average_time:.4f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chneE_4sjlJk"
   },
   "source": [
    "Please note that in practice frequent queries will be cached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxd0oibM62YF"
   },
   "source": [
    "# Vector database search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgQd-D0dKhUo"
   },
   "source": [
    "However, the main concern is that often your data will be too large to fit into RAM, let alone VRAM, so an option better than numpy array search is needed. Also, we can not afford to spend O(n) search time for each request.\n",
    "\n",
    "Vector databases i.e. databases specifically designed to store vectors and conduct fast retrieval operations solve the speed problem at the expense of having to keep the storage in RAM. In practice, a collections of machines (shards) may host different chunks of data in an interconnected way.\n",
    "\n",
    "Below is an already implemented way of building an embedding storage. You may want to look it through carefully.\n",
    "\n",
    "You can read more about [FAISS](https://faiss.ai/index.html) library and [HNSW](https://arxiv.org/abs/1603.09320), which is, in short, a k-nearest neighbors search on steroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "JBw4-UpywHUS"
   },
   "outputs": [],
   "source": [
    "def build_faiss_hnsw_index(dimension: int, ef_construction: int = 200, M: int = 32) -> faiss.IndexHNSWFlat:\n",
    "    \"\"\"\n",
    "    Builds a FAISS HNSW index for cosine similarity.\n",
    "\n",
    "    This function initializes a HNSW (Hierarchical Navigable Small World) index for efficient approximate nearest neighbor search\n",
    "    based on cosine similarity, using the FastText model's normalized embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        dimension (int): Dimensionality of the embeddings (size of each vector).\n",
    "        ef_construction (int, optional): Trade-off parameter between index construction speed and accuracy. Default is 200.\n",
    "        M (int, optional): Number of neighbors in the graph, controlling the memory and accuracy tradeoff. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        index (faiss.IndexHNSWFlat): Initialized FAISS HNSW index.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexHNSWFlat(dimension, M)  # HNSW index\n",
    "    index.hnsw.efConstruction = ef_construction  # Construction accuracy\n",
    "    index.metric_type = faiss.METRIC_INNER_PRODUCT  # Cosine similarity via normalized vectors\n",
    "    return index\n",
    "\n",
    "\n",
    "def populate_faiss_index(index: faiss.Index, model, dataset_path: str, batch_size: int = 10000):\n",
    "    \"\"\"\n",
    "    Populates the FAISS HNSW index with normalized embeddings from the dataset.\n",
    "\n",
    "    This function reads the dataset line by line, preprocesses each question, computes its embedding,\n",
    "    and adds the embeddings to the FAISS index in batches.\n",
    "\n",
    "    Parameters:\n",
    "        index (faiss.Index): FAISS index to populate.\n",
    "        model: Trained FastText model used to generate embeddings.\n",
    "        dataset_path (str): Path to the dataset file (one question per line).\n",
    "        batch_size (int, optional): Number of questions to process at a time. Default is 10000.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        buffer = []\n",
    "        for line in f:\n",
    "            # Preprocess the line and get its embedding\n",
    "            question = preprocess_line(line.strip())\n",
    "            embedding = get_text_embedding(question, model)\n",
    "            buffer.append(embedding)\n",
    "\n",
    "            # Add embeddings to the index in batches\n",
    "            if len(buffer) >= batch_size:\n",
    "                index.add(np.array(buffer, dtype=np.float32))\n",
    "                buffer = []  # Clear the buffer after adding\n",
    "\n",
    "        # Add remaining embeddings (if any) after loop finishes\n",
    "        if buffer:\n",
    "            index.add(np.array(buffer, dtype=np.float32))\n",
    "\n",
    "\n",
    "def search_faiss_index(embeddings_storage: faiss.Index, query: str, model, k: int = 5) -> Tuple[List[int], List[float]]:\n",
    "    \"\"\"\n",
    "    Searches the FAISS index for the closest matches to a query.\n",
    "\n",
    "    This function computes the embedding for the input query, searches the FAISS index for the most similar questions,\n",
    "    and returns the indices and distances (cosine similarity) of the top-k matches.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings_storage (faiss.Index): FAISS index to search.\n",
    "        query (str): The input query string to search for.\n",
    "        model: Trained FastText model used to generate embeddings.\n",
    "        k (int, optional): Number of closest matches to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[float]]:\n",
    "            - List[int]: A list of indices of the top-k most similar questions.\n",
    "            - List[float]: A list of distances (cosine similarity) of the top-k results.\n",
    "    \"\"\"\n",
    "    # Preprocess and normalize the query embedding\n",
    "    query_embedding = get_text_embedding(query, model)\n",
    "\n",
    "    # Search the embeddings_storage (FAISS index)\n",
    "    top_k_distances, top_k_indices = embeddings_storage.search(np.array([query_embedding], dtype=np.float32), k)\n",
    "\n",
    "    # Match return format with that used in numpy storage search\n",
    "    top_k_indices_list = top_k_indices[0].tolist()\n",
    "    top_k_distances_list = top_k_distances[0].tolist()\n",
    "\n",
    "    return top_k_indices_list, top_k_distances_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jyc6V3LyHkq"
   },
   "source": [
    "Building a faiss may take some time, it's ok. In some sense, we trade computations during index constuction for query time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "JdcPB-XRw-RE"
   },
   "outputs": [],
   "source": [
    "# Define the dimensions of the embedding vectors\n",
    "embedding_dimension = finetuned_model.vector_size  # Depends on the FastText model\n",
    "\n",
    "# Build the HNSW index\n",
    "hnsw_index = build_faiss_hnsw_index(embedding_dimension)\n",
    "\n",
    "# Populate the index from the quora_processed.txt dataset\n",
    "populate_faiss_index(hnsw_index, finetuned_model, \"quora_processed.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UnCFwTz5-3x"
   },
   "source": [
    "Note that we built a FAISS index around Euclidian distance, not cosine similarity, to be completely honest:\n",
    "\n",
    "$$\n",
    "cosine \\space similarity = 1 - \\frac{euclidian \\space distance^2}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZGDk2Pc9brd"
   },
   "source": [
    "Note that the results obtained may not entirely repeat a numpy-based search since HNSW index is built upon approximate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxEXNNtexQOo",
    "outputId": "bd2bfb01-2e80-4cab-f88d-6e4dbd11bfbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I find inner peace?\n",
      "\n",
      "Top Matches:\n",
      "1. How can I find passion? (Similarity: -0.0108)\n",
      "2. How can I create inner peace? (Similarity: -0.0119)\n",
      "3. How can I find a career mentor? (Similarity: -0.0128)\n",
      "4. How can I find inspiration? (Similarity: -0.0134)\n",
      "5. How can I find a successful mentor? (Similarity: -0.0135)\n",
      "6. How can I find a startup mentor? (Similarity: -0.0135)\n",
      "7. How can I find a programmer mentor? (Similarity: -0.0140)\n",
      "8. How can I find a job? (Similarity: -0.0163)\n",
      "9. How can I find a trustworthy distributor？? (Similarity: -0.0171)\n",
      "10. How can I find a JavaScript programmer mentor? (Similarity: -0.0175)\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_text = \"How can I find inner peace?\"\n",
    "\n",
    "top_k_indices, top_k_similarities = fetch_and_display_closest_match(query_function=search_faiss_index,\n",
    "                                                                    query=query_text, model=finetuned_model,\n",
    "                                                                    embeddings_storage=hnsw_index,\n",
    "                                                                    k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYAdJNkO4dGA",
    "outputId": "41a4e358-9239-45cf-947f-7b25dcc70e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0053 seconds.\n",
      "Average time: 0.0001 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(iterations):\n",
    "    _, _ = search_faiss_index(hnsw_index, query_text, finetuned_model, k=10)\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "average_time = time_elapsed / iterations\n",
    "\n",
    "print(f'Time elapsed: {time_elapsed:.4f} seconds.')\n",
    "print(f'Average time: {average_time:.4f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFmyOsoC6qez"
   },
   "source": [
    "Note that we achieved a two orders of magnitude speed-up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkItnFGLmY2Z"
   },
   "source": [
    "What to do next?\n",
    "\n",
    "As far as embeddings are concerned, you might want to improve how a single word embedding is obtained or introduce individual embeddings weighings with e.g. TF-IDF. Further in the course, you will learn better (yet substantially computationally heavy) ways to represent an arbitrary text other than aggregating standalone vectors. However, word-level embeddings are still a decent option if computational resources are limited.\n",
    "\n",
    "As far as query engine is concerned, caching is a great option to get rid of redundant queries. For example, take a look at [Redis](https://redis.io/learn/howtos/solutions/microservices/caching)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "6-nlp-58CPPIPe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
